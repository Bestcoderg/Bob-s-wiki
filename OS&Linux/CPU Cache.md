# CPU Cache

# 概述

Cache 是一种又小又快的存储器。由于CPU与内存速度差距很大，并在逐步扩大。Cache的出现便是为了弥合Memory与CPU之间的速度差距。

![Untitled](CPU%20Cache%20dc3c67120bfb4c389d3df957be390e3c/Untitled.png)

## L1 / L2 / L3

![Untitled](CPU%20Cache%20dc3c67120bfb4c389d3df957be390e3c/Untitled%201.png)

现代计算机CPU中有好几级的缓存，老的 CPU 会有两级内存（L1 和 L2），新的 CPU 会有三级内存（L1，L2，L3 ）。

其中：

- L1 缓存分成两种，一种是指令缓存，一种是数据缓存。L2 缓存和 L3 缓存不分指令和数据。
- L1 和 L2 缓存在每一个 CPU 核中，L3 则是所有 CPU 核心共享的内存。
- L1、L2、L3 的越离 CPU 近就越小，速度也越快，越离 CPU 远，速度也越慢。

再往后面就是内存，内存的后面就是硬盘。我们来看一些他们的速度：

- L1 的存取速度：**4 个 CPU 时钟周期**
- L2 的存取速度：**11 个 CPU 时钟周期**
- L3 的存取速度：**39 个 CPU 时钟周期**
- RAM 内存的存取速度 **：107 个 CPU 时钟周期**

我们的数据就从内存向上，先到 L3，再到 L2，再到 L1，最后到寄存器进行 CPU 计算。为什么会设计成三层？这里有下面几个方面的考虑：

- 一个方面是物理速度，如果要更大的容量就需要更多的晶体管，除了芯片的体积会变大，更重要的是大量的晶体管会导致速度下降，因为访问速度和要访问的晶体管所在的位置成反比，也就是当信号路径变长时，通信速度会变慢。这部分是物理问题。
- 另外一个问题是，多核技术中，数据的状态需要在多个 CPU 中进行同步，并且，我们可以看到，cache 和 RAM 的速度差距太大，所以，多级不同尺寸的缓存有利于提高整体的性能。

## Cache Line

在详解 Cache 之前。我们需要要解一个术语 **Cache Line**。我们前面提到过内存的管理是以页为单位管理的，对于 CPU 来说也是如此，它是不会一个字节一个字节的加载的，因为这非常没有效率，一般来说都是要一块一块的加载的。对于这样的一块一块的数据单位作为我们管理cache的最小单位，术语叫 **Cache Line。**一般来说，一个**主流的 CPU 的 Cache Line 是 64 Bytes**（也有的 CPU 用 32Bytes 和 128Bytes），64 Bytes 也就是 16 个 32 位的整型，这就是 CPU 从内存中捞数据上来的最小数据单位。(比如：L1 有 32KB，那么，32KB/64B = 512 个 Cache Line)

**CPU从Cache数据的最小单位是字节，Cache从Memory拿数据的最小单位（这里不讲嵌入式系统）是64Bytes，Memory从硬盘拿数据通常最小是4092Bytes。**

# Cache 映射策略

既然我们提到过 Cache 是为了弥合主存与 CPU 的速度差距而生的，那么为了把信息放到 Cache 中，必须应用某种函数把主存地址定位到 Cache 中，这称为**地址映射**。在信息按这种映射关系装入 Cache 后，CPU 执行程序时，会将程序中的主存地址变换成 Cache 地址，这个变换过程叫做地址变换。**Cache 的地址映射方式有直接映射、全相联映射和组相联映射。**

## 直接映射

直接映射的 Cache 组织如下图所示。**主存中的一个块只能映射到 Cache 的某一特定块中去，可以理解为一次取模**。例如，主存的第 0 块、第 16 块、……、第 2032 块，只能映射到 Cache 的第 0 块；而主存的第 1 块、第 17 块、……、第 2033 块，只能映射到 Cache 的第 1 块……。

![Untitled](CPU%20Cache%20dc3c67120bfb4c389d3df957be390e3c/Untitled%202.png)

**直接映射是最简单的地址映射方式，它的硬件简单，成本低，地址变换速度快，而且不涉及替换算法问题。但是这种方式不够灵活，Cache 的存储空间得不到充分利用，每个主存块只有一个固定位置可存放，容易产生冲突，使 Cache 效率下降，因此只适合大容量 Cache 采用。**例如，如果一个程序需要重复引用主存中第 0 块与第 16 块，最好将主存第 0 块与第 16 块同时复制到 Cache 中，但由于它们都只能复制到 Cache 的第 0 块中去，即使 Cache 中别的存储空间空着也不能占用，因此这两个块会不断地交替装入 Cache 中，导致命中率降低。

## 全相联映射

全相联映射的 Cache 意味着**主存中任何一块都可以映射到 Cache 中的任何一块位置上**

![Untitled](CPU%20Cache%20dc3c67120bfb4c389d3df957be390e3c/Untitled%203.png)

全相联映射方式比较灵活，主存的各块可以映射到 Cache 的任一块中，**Cache 的利用率高，块冲突概率低**，只要淘汰 Cache 中的某一块，即可调入主存的任一块。但是，**由于 Cache 比较电路的设计和实现比较困难，这种方式只适合于小容量 Cache 采用。**

## 组相联映射

组相联映射实际上是直接映射和全相联映射的折中方案，其组织结构如下图所示。**主存和 Cache 都分组，主存中一个组内的块数与 Cache 中的分组数相同，组间采用直接映射，组内采用全相联映射**。也就是说，将 Cache 分成 u 组，每组 v 块，主存块存放到哪个组是固定的，至于存到该组哪一块则是灵活的。例如，主存分为 256 组，每组 8 块，Cache 分为 8 组，每组 2 块(后面叫做 2 路)。

![Untitled](CPU%20Cache%20dc3c67120bfb4c389d3df957be390e3c/Untitled%204.png)

主存中的各块与 Cache 的组号之间有固定的映射关系，但可自由映射到对应 Cache 组中的任何一块(路)。例如，主存中的第 0 块、第 8 块…… 均映射于 Cache 的第 0 组，但可映射到 Cache 第 0 组中的第 0 块或第 1 块；主存的第 1 块、第 9 块…… 均映射于 Cache 的第 1 组，但可映射到 Cache 第 1 组中的第 2 块或第 3 块。

常采用的组相联结构 Cache，**每组内有 2、4、8、16 块，称为 2 路、4 路、8 路、16 路组相联 Cache**。组相联结构 Cache 是前两种方法的折中方案，**适度兼顾二者的优点，尽量避免二者的缺点，因而得到普遍采用。**

## Cache 组织形式 \ 实现

上文我们提到了Cache映射的三种方式，N-Way组相联是当前多数处理器选择的解决方案。我们来具体探究一下N-Way组相在物理上的实现。

下图展示了现代 Intel 处理器的 CPU cache 是如何组织的，展示了Core 的 L1 cache 是如何被访问的：

![Untitled](CPU%20Cache%20dc3c67120bfb4c389d3df957be390e3c/Untitled%205.png)

在 cache 中的数据是以缓存线（ine）为单位组织的，一条缓存线对应于内存中一个连续的字节块。这个 cache 使用了 64 字节的缓存线。**这些线被保存在 cache bank 中，也叫路（way）**。每一路都有一个专门的目录（directory）用来保存一些登记信息。你可以把每一路连同它的目录想象成电子表格中的一列，**而表的一行构成了 cache 的一组（set）。列中的每一个单元（cell）都含有一条缓存线**，由与之对应的目录单元跟踪管理。图中的 **cache 有 64 组、每组 8 路，因此有 512 个含有缓存线的单元，合计 32KB 的存储空间（注意组与路的区别）**。

**在 cache 眼中，物理内存被分割成了许多 4KB 大小的物理内存页（page）（恰好每一路是一个 page？）**。**每一页都含有 4KB / 64 bytes == 64 条缓存线（所以我们将4K作为我们一路的大小，将cache的空间分为了64组）**。在一个 4KB 的页中，第 0 到 63 字节是第一条缓存线，第 64 到 127 字节是第二条缓存线，以此类推。每一页都重复着这种划分，所以第 0 页第 3 条缓存线与第 1 页第 3 条缓存线是不同的。

**内存中一条给定的 cache line 只能被保存在一个特定的组cache way/bank 中**。所以，任意物理内存页的第 0 条缓存线（页内第 0 到 63 字节）必须存储到第 0 组，第 1 条缓存线存储到第 1 组，以此类推。每一组有 8 个单元可用于存储它所关联的缓存线，从而形成一个 8 路关联的组（8-way associative set）。当访问一个内存地址时，**地址的第 6 到 11 位（译注：组索引，因为有 64 个组，所以 6bit 索引）指出了在 4KB 内存页中缓存线的编号**，从而决定了即将使用的缓存组。举例来说，物理地址 0x800010a0 的组索引是 000010，所以此地址的内容一定是在第 2 组中缓存的。

### 在组中检测cache line匹配

现在我们已经成功将内存与cache对应(映射)起来了！但是还有一个问题，就是在确定了组后要找出一组中哪个单元包含了想要的信息，如果有的话。这就到了**缓存目录**登场的时刻。**每一个缓存线都被其对应的目录单元做了标记（tag）；这个标记就是一个简单的内存页编号，指出缓存线来自于哪一页。由于处理器可以寻址 64GB 的物理 RAM**，所以总共有 64GB / 4KB == 224 个内存页，需要 24 位来保存标记。前例中的物理地址 0x800010a0 对应的页号为 524,289。

![Untitled](CPU%20Cache%20dc3c67120bfb4c389d3df957be390e3c/Untitled%206.png)

由于我们只需要去查看某一组中的 8 路，所以查找匹配标记是非常迅速的；事实上，**从电学角度讲，所有的标记是同时进行比对的**，我用箭头来表示这一点。如果此时正好有一条具有匹配标签的有效缓存线，我们就获得一次缓存命中（**cache hit**）。否则，这个请求就会被转发的 L2 cache，如果还没匹配上就再转发给主系统内存。

通过应用各种调节尺寸和容量的技术，Intel 给 CPU 配置了较大的 L2 cache，但其基本的设计都是相同的。比如，你可以将原先的缓存增加 8 路而获得一个 64KB 的缓存；再将组数增加到 4096，每路可以存储 256KB。经过这两次修改，就得到了一个 4MB 的 L2 cache。在此情况下，需要 18 位来保存标记，12 位保存组索引；缓存所使用的物理内存页的大小与其一路的大小相等。（译注：有 4096 组，就需要 lg(4096)==12 位的组索引，cache line依然是 64 字节，所以一路有 4096*64B==256KB 字节；在 L2 cache 眼中，内存被分割为许多 256KB 的块，所以需要 lg(64GB/256KB)==18 位来保存标记。）

如果有一组已经被放满了，那么在另一条缓存线被存储进来之前，已有的某一条则必须被腾空（evict）。为了避免这种情况，对运算速度要求较高的程序就要尝试仔细组织它的数据，使得内存访问均匀的分布在已有的缓存线上。举例来说，假设程序中有一个数组，元素的大小是 512 字节，其中一些对象在内存中相距 4KB。这些对象的各个字段都落在同一缓存线上，并竞争同一缓存组。如果程序频繁的访问一个给定的字段（**slab 着色操作便是为了解决这个问题 [Linux 内存分配机制](Linux%20%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E6%9C%BA%E5%88%B6%209d21f14218864a9a9fbe85ca969926c4.md)** ），那么这个组看起来就好像一直是被填满的，缓存开始变得毫无意义，因为缓存线一直在重复着腾空与重新载入的步骤。在我们的例子中，由于组数的限制，L1 cache 仅能保存 8 个这类对象的虚函数表。这就是组相联策略的折中所付出的代价：即使在整体缓存的使用率并不高的情况下，由于组冲突，我们还是会遇到缓存缺失的情况。然而，鉴于计算机中各个存储层次的相对速度，不管怎么说，大部分的应用程序并不必为此而担心。

一个内存访问经常由一个**虚拟地址**发起，所以 L1 cache 需要依赖分页单元（paging unit）来求出物理内存页的地址，以便用于缓存标记。与此相反，组索引来自于线性地址的低位，所以不需要转换就可以使用了（由于在小于一页4K的情况下虚拟地址4K以下的地址与物理地址是一致的，在我们的例子中为第 6 到 11 位）。因此 **L1 cache 是物理标记但虚拟索引的**（physically tagged but virtually indexed），从而帮助 CPU 进行并行的查找操作。**因为 L1 cache 的一路绝不会比 MMU 的一页还大，所以可以保证一个给定的物理地址位置总是关联到同一组，即使组索引是虚拟的**。在另一方面 **L2 cache 必须是物理标记和物理索引的，因为它的一路比 MMU 的一页要大**。但是，当一个请求到达 L2 cache 时，物理地址已经被 L1 cache 准备（resolved）完毕了，所以 L2 cache 会工作得很好。

# Cache 替换策略

Cache里存的数据是Memory中的**常用**数据一个拷贝，Cache比较小，不可以缓存Memory中的所有数据。当Cache存满后，再需要存入一个新的条目时，就需要把一个旧的条目从缓存中拿掉，这个过程称为evict，一个被evict的条目称为victim。缓存管理单元通过一定的算法决定哪些数据有资格留在Cache里，哪些数据需要从Cache里移出去。这个策略称为**替换策略（replacement policy)**。最简单的替换策略称为LRU(least recently used)，即Cache管理单元记录每个Cache line最近被访问的时间，每次需要evict时，选最近一次访问时间最久远的那一条做为victim。在实际使用中，LRU并不一定是最好的替换策略，在CPU设计的过程中，通常会不段对替换策略进行改进，每一款芯片几乎都使用了不同的替换策略。

# Cache 写入策略 \ 一致性策略

CPU需要读写一个地址的时候，先去Cache中查找，如果数据不在Cache中，称为Cache miss，就需要从Memory中把这个地址所在的那个Cache line上的数据加载到Cache中。然后再把数返回给CPU。这时会伴随着另一个Cache 条目成为victim被替换出去。如果CPU需要访问的数据在Cache中，则称为Cache hit。

针对写操作，有两种写入策略，分别为 write back 和 write through 。write through策略下，数据直接同时被写入到Memory中，在write back策略中，数据仅写到Cache中，此时Cache中的数据与Memory中的数据不一致，Cache中的数据就变成了脏数据(dirty)。如果其他部件（DMA， 另一个核）访问这段数据的时候，就需要通过 **Cache一致性协议** 保证取到的是最新的数据。另外这个Cache被替换出去的时候就需要写回到内存中。

## **MESI 协议**

**`MESI`**（`Modified Exclusive Shared Or Invalid`）(也称为伊利诺斯协议，是因为该协议由伊利诺斯州立大学提出）是一种广泛使用的支持 写回write back 策略的缓存一致性协议。

如果我们采用 write back 策略，不免的我们将要面临缓存一致性的问题，实际上对于这种策略，我们面临的核心问题就只有两个：

- 单个 CPU 更改数据后导致的多个缓存的缓存一致性问题
- 多个 CPU 同时写数据导致的写顺序问题

针对这两个问题我们可以通过两个一致性来满足我们的需求：

- cpu 的读操作必须能读到修改后的新值
- cpu 对同一缓存行的写操作不能同时发生

### 解决一致性问题的两种写策略

而对于这两点要求，我们可以通过两种写策略来实现：

- **写无效：**任一 cpu 在写某个数据时，会使其他 cpu 的缓存中的拷贝失效。在这种策略下，cpu 写数据前，会先通过总线使其他缓存中拷贝失效，这就相当于取得了该数据的唯一访问权，因此接下来写数据就不会导致一致性问题。而其他 cpu 要读该数据时，由于该数据已失效，就必须先从其他缓存或内存中获得最新的值。
- **写更新：**任一 cpu 在写数据时，会通过总线广播使其他缓存中的拷贝进行更新。

由于写无效策略在性能上优于写更新策略，如对同一数据多个写而中间无读的情况，写更新需要多次写广播操作，而在写无效协议下只需一次写无效操作，因此，在基于总线的多核计算机中，写无效策略成为大多数系统设计的选择，所以我们可以看到MESI中有一个”I“的状态即Invalid失效。

### 监听协议

上述的 2 种缓存写策略解决了导致缓存一致性的核心问题，但是还不够，若要完整的解决一致性问题，势必需要更加完整的方案。监听协议就是其中必不可少的一个策略。

监听协议使用共享总线连接多个 cpu 的私有缓存和内存，共享总线保证所有处理器内核的数据请求串行执行。任何处理器发出的数据请求将被广播到所有的 cpu 缓存控制器，所有 cpu 的缓存控制器都时刻监视着总线。如果收到读请求，数据所有者将把有效数据返回给发出此请求的 cpu；如果收到写请求，拥有有效副本的 cpu 便无效或更新本 cpu 上的数据，数据所有者将把有效数据返回到发出此请求的 cpu。数据所有者可能是 cpu，也可能是内存

如下图所示情景，cpu1 发生写数据前，会把写请求广播到总线，此时 cpu2 的缓存控制器监听到此总线请求便会将 x 置为无效并给总线广播回复信号，cpu1 收到回复信号确认其他 cpu 已将 x 的拷贝全部置为无效后，才会修改 x。（如果 cpu1，cpu2 同时修改 x，则必然其中一个 cpu 先获取总线控制权，之后它使 cpu2 的缓存上的拷贝失效。）接下来如果 cpu2 读取 x，因为 x 是无效的状态，所以会向总线广播读请求，cpu1 监听到该总线请求后会把 x 的内容通过总线传给 cpu2 的缓存。

![Untitled](CPU%20Cache%20dc3c67120bfb4c389d3df957be390e3c/Untitled%207.png)

### **MESI 协议中的状态**

`CPU`中每个缓存行（`caceh line`) 使用 4 种状态进行标记（使用额外的两位 (`bit`) 表示):

- **M: 被修改（Modified)**

该缓存行只被缓存在该`CPU`的缓存中，并且是被修改过的（`dirty`), 即与主存中的数据不一致，该缓存行中的内存需要在未来的某个时间点（允许其它`CPU`读取请主存中相应内存之前）写回（`write back`）主存。当被写回主存之后，该缓存行的状态会变成独享（`exclusive`) 状态。

- **E: 独享的（Exclusive)**

该缓存行只被缓存在该`CPU`的缓存中，它是未被修改过的（`clean`)，与主存中数据一致。该状态可以在任何时刻当有其它`CPU`读取该内存时变成共享状态（`shared`)。

同样地，当`CPU`修改该缓存行中内容时，该状态可以变成`Modified`状态。

**S: 共享的（Shared)**

该状态意味着该缓存行可能被多个`CPU`缓存，并且各个缓存中的数据与主存数据一致（`clean`)，当有一个`CPU`修改该缓存行中，其它`CPU`中该缓存行可以被作废（变成无效状态（`Invalid`））。

**I: 无效的（Invalid）**

该缓存是无效的（可能有其它`CPU`修改了该缓存行）。

---

这些一致性状态通过高速缓存和内存之间的通信进行维护。 当缓存中的某行被读或写时，或者当缓存通过总线接收到其他缓存发出的读写信号时，它需要据此来做出动作并调整自己的状态。

当缓存收到 cpu 的读请求时，如果一个缓存行处于 “M” 或“S”状态，则它会直接提供数据。但如果缓存行尚未被加载到缓存 (处于“I” 状态)，则在加载该缓存行之前，cpu 中的缓存控制器会向总线广播这个读请求，在收到这个广播后，其他缓存中处于 "E" 状态的拷贝直接修改为 “S" 状态就可以了；而处于“M” 状态的拷贝，则会将数据地址广播到总线，以供读请求的 cpu 拿到新数据，并且写回内存，在提供数据后，该拷贝修改为 “S” 状态。

当缓存收到 cpu 的写请求时，如果这缓存行处于 "M" 状态，则缓存只需要修改本地的数据。 如果缓存行处于 "S" 状态，则必须先将其他缓存中的拷贝置为 “I” 状态后，在修改本地的数据。 如果缓存行处于 "I" 状态，则其他处于”S"状态的拷贝只需将拷贝置为 “I” 状态；如果有一个拷贝处于"M"状态，那么它必须先将数据通过总线提供给请求数据的缓存，并写回内存，然后将拷贝置为 “I” 状态。如果此时缓存尚未装载该缓存行的数据，则修改前要先将其从内存中读取。在数据被修改之后，缓存行处于"M" 的状态。

MESI 协议可以看作是一个有限状态机

![Untitled](CPU%20Cache%20dc3c67120bfb4c389d3df957be390e3c/Untitled%208.png)

# Cache L1 L2 L3速度差别

L1 的存取速度：**4 个 CPU 时钟周期**

L2 的存取速度：**11 个 CPU 时钟周期**

L3 的存取速度：**39 个 CPU 时钟周期**

RAM 内存的存取速度 **：107 个 CPU 时钟周期**

## L1/L2速度差距

- **存储容量不同导致的速度差异、**
  
    L1的容量通常比L2小，容量大的SRAM访问时间就越长，同样制程和设计的情况下，访问延时与容量的开方大致是成正比的。
    
- **离CPU远近导致的速度差异**
  
    通常L1 Cache离CPU核心需要数据的地方更近，而L2 Cache则处于边缓位置，访问数据时，L2 Cache需要通过更远的铜线，甚至更多的电路，从而增加了延时。
    
    **L1 Cache分为ICache（指令缓存）和DCache(数据缓存）**,指令缓存ICache通常是放在CPU核心的**指令预取单元**附近的，数据缓存DCache通常是放在CPU核心的**load/store单元**附近。而L2 Cache是放在CPU pipeline之外的。
    
    为什么不把L2 Cache也放在很近的地方呢？由于Cache的容量越大，面积越大，相应的边长的就越长（假设是正方形的话），总有离核远的。
    
    下面的图并不是物理上的图，只是为大家回顾一下CPU的pipe line。另外需要注意的是这张图里展示了一个二级的DTLB结构，和一级的ITLB。
    

![Untitled](CPU%20Cache%20dc3c67120bfb4c389d3df957be390e3c/Untitled%209.png)

- **制程不同的造成的速度差异**
  
    在实际设计制造时，针对L1/L2的不同角色，L1更加注重速度， L2更加注重节能和容量。在制程上这方面有体现，（但我不懂。。。。）。在设计时，这方面的有体现：首先， L1 Cache都是N路组相联的，N路组相联的意思时，给定一个地址，N个Cache单元同时工作，取出N份tag和N份数据，然后再比较tag，从中选出hit的那一个采用，其它的丢弃不用。这种方式一听就很浪费，很不节能。
    
    另外，L2 Cache即便也是N路组相联的，但它是先取N个tag，然后比对tag后发现cache hit之后再把对应的数据取出来。由于L2是在L1 miss之后才会访问，所以L2 cache hit的概率并不高，访问的频率也不高，而且有前面L1抵挡一下，所以它的延迟高点也无所谓，L2容量比较大，如果数据和tag一起取出来，也比较耗能。
    
    通常专家都将L1称为latency filter, L2称为bandwidth filter。
    

## L3 Cache

L1/L2 Cache通常都是每个CPU核心一个（x86而言，ARM一般L2是为一个簇即4个核心共享的），这意味着每增加一个CPU核心都要增加相同大小的面积，即使各个CPU核心的L2 Cache有很多相同的数据也只能各保存一份，因而一个所有核心共享的L3 Cache也就有必要了。

L3 Cache通常都是各个核心共享的，而且DMA之类的设备也可以用

## **逻辑Cache和物理Cache**

Cache在系统中的位置根据与MMU的相对位置不同，分别称为logical Cache和physical cache。

Logical Cache接受的是逻辑地址，物理Cache接受的是物理地址

![Untitled](CPU%20Cache%20dc3c67120bfb4c389d3df957be390e3c/Untitled%2010.png)

# **CACHE 接入系统的体系结构**

## **侧接法**

像入出设备似的连接到总线上，优点是结构简单，成本低，缺点是不利于降低总线占用率。

![Untitled](CPU%20Cache%20dc3c67120bfb4c389d3df957be390e3c/Untitled%2011.png)

## **隔断法**

把原来的总线打断为两段，使 CACHE 处在两段之间，优点是有利于提高总线利用率，支持总线并发操作，缺点是结构复杂，成本较高。

![Untitled](CPU%20Cache%20dc3c67120bfb4c389d3df957be390e3c/Untitled%2012.png)